{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM 교통량 예측(base).ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOvw3Nu2gLIp46uzzJJVKgI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kimjibeom/LSTM/blob/main/LSTM_%EA%B5%90%ED%86%B5%EB%9F%89_%EC%98%88%EC%B8%A1(base).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tivfXVXWpCn"
      },
      "source": [
        "# Google Drive 마운트 하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfHYmC3AWsR2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vlfogbwWyh7"
      },
      "source": [
        "# 경로 설정\n",
        "import os\n",
        "os.chdir('/content/drive/My Drive/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quCVKTUaXHxQ"
      },
      "source": [
        "# 라이브러리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvcQGjnyP121"
      },
      "source": [
        "from keras import backend\n",
        "from pandas import DataFrame\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.preprocessing.sequence import TimeseriesGenerator\n",
        "from keras.models import model_from_json\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6reQl_RFV1n9"
      },
      "source": [
        "df = pd.read_csv('.csv')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98qZxTn-XRTi"
      },
      "source": [
        "# 스케일링"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-GjinGoXTzV"
      },
      "source": [
        "def minmax_scaler(X, minmax_range):\n",
        "  scaled = (X - minmax_range[0]) / (minmax_range[1] - minmax_range[0])\n",
        "  return scaled"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkKdns5UX3sc"
      },
      "source": [
        "look_back 인자는 얼마만큼의 이전 수치를 데이터로 만들것인가 결정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-UgeHt3YBLS"
      },
      "source": [
        "# 192는 수정필요\n",
        "def create_dataset(data, data2, look_back=192):\n",
        "  dataX, dataY = [], []\n",
        "  for i in range(len(data)-2*look_back):\n",
        "    dataX.append(data[i:(i+look_back)])\n",
        "    dataY.append(data2[i+2*(look_back)])\n",
        "  return np.array(dataX), np.array(dataY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8NI0dvLaTsz"
      },
      "source": [
        "df = df.interpolate(method='linear', limint_direction='both', axis=0)\n",
        "\n",
        "train_set = df.iloc[:int(len(df)*0.8)]\n",
        "test_set = df.iloc[int(len(df)*0.8):]\n",
        "\n",
        "print(f'전체 데이터: {len(df)}')\n",
        "print(f'학습 데이터: {len(train_set)}')\n",
        "print(f'시험 데이터: {len(test_set)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jK0vgPfoa3_A"
      },
      "source": [
        "# 점유율\n",
        "pw = np.array(train_set['점유율'])\n",
        "pw = pw.reshape(pw.shape[0],1)\n",
        "# 도로총교통량\n",
        "tf = np.array(train_set['도로총교통량'])\n",
        "tf = tf.reshape(tf.shape[0],1)\n",
        "# 평균통행속도\n",
        "spd = np.array(train_set['평균통행속도'])\n",
        "spd = spd.reshape(spd.shape[0],1)\n",
        "\n",
        "pw = minmax_scaler(pw, [min(pw), max(pw)])\n",
        "tf = minmax_scaler(tf, [0,6000])\n",
        "spd = minmax_scaler(spd, [0,200])\n",
        "\n",
        "total = np.hstack([pw,tf,spd])\n",
        "X_train, y_train = create_dataset(total,pw)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3gxtd6kd2es"
      },
      "source": [
        "# 모델"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8r8JxHcd1zt"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(32,input_shape=(X_train.shape[1],4),return_sequences=True))\n",
        "model.add(LSTM(32,input_shape=(X_train.shape[1],4)))\n",
        "# model.add(TimeDistributed(Dense(1)))\n",
        "model.add(Dense(1))\n",
        "\n",
        "model.compile(loss='mae', optimizer='adam')\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=384, verbose=2, shuffle=False)\n",
        "\n",
        "from keras.models import model_from_json\n",
        "\n",
        "model_json = model.to_json()\n",
        "jsonname = 'lstm_model.json'\n",
        "h5name = 'lstm_model.h5'\n",
        "with open(jsonname,\"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "\n",
        "model.save_weights(h5name)\n",
        "print(\"Saved model to disk\")\n",
        "print('finished!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLgvN-aPiYJO"
      },
      "source": [
        "# 테스트"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLBdxakyiZiF"
      },
      "source": [
        "def inverse_scaler(X,minmax_range):\n",
        "    inv = X * (minmax_range[1]-minmax_range[0]) + minmax_range[0]\n",
        "    return inv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4mYjP64ib6L"
      },
      "source": [
        "pw_range = [min(df['점유율']), max(df['점유율'])]\n",
        "# 점유율\n",
        "pw = np.array(train_set['점유율'])\n",
        "pw = pw.reshape(pw.shape[0],1)\n",
        "# 도로총교통량\n",
        "tf = np.array(train_set['도로총교통량'])\n",
        "tf = tf.reshape(tf.shape[0],1)\n",
        "# 평균통행속도\n",
        "spd = np.array(train_set['평균통행속도'])\n",
        "spd = spd.reshape(spd.shape[0],1)\n",
        "\n",
        "pw = minmax_scaler(pw, pw_range)\n",
        "tf = minmax_scaler(tf, [0,6000])\n",
        "spd = minmax_scaler(spd, [0,200])\n",
        "\n",
        "Test = np.hstack([pw,tf,spd])\n",
        "\n",
        "json_file = open('lstm_model.json',\"r\")\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "model = model_from_json(loaded_model_json)\n",
        "model.load_weights('lstm_model.h5')\n",
        "model.compile(loss='mse', optimizer='adam')\n",
        "\n",
        "predictions = []\n",
        "y_test = []\n",
        "\n",
        "i = 0; cnt = 0\n",
        "while (i < len(Test)-384):\n",
        "    X_test, test_y = create_dataset(Test[i:i+385],pw[i:i+385],192)\n",
        "    pred = model.predict(X_test)\n",
        "#     print(pred, end=' ')\n",
        "    predictions.append(pred)\n",
        "    y_test.append(test_y)\n",
        "\n",
        "    i += 1\n",
        "    cnt += 1\n",
        "##### inverse transform ##############\n",
        "y_test = inverse_scaler(np.array(y_test),pw_range)\n",
        "predictions = inverse_scaler(np.array(predictions),pw_range)\n",
        "y_test = np.vstack(y_test).squeeze()\n",
        "predictions = np.vstack(predictions).squeeze()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}